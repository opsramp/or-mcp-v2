# OpsRamp AI Agent - Comprehensive Testing Framework

This directory contains a comprehensive testing framework for the OpsRamp AI Agent, organized to test both **Integration** and **Resources** functionality with real API evidence collection.

## ðŸ—ï¸ Directory Structure

```
tests/
â”œâ”€â”€ integration/                    # Integration Management Testing
â”‚   â”œâ”€â”€ test_data/
â”‚   â”‚   â”œâ”€â”€ basic_integration_prompts.txt
â”‚   â”‚   â”œâ”€â”€ advanced_integration_prompts.txt
â”‚   â”‚   â””â”€â”€ integration_scenarios.json
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ run_integration_tests.py
â”‚   â”‚   â””â”€â”€ validate_integration_results.py
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ logs/                   # Test execution logs
â”‚       â”œâ”€â”€ payloads/              # Real API payloads
â”‚       â””â”€â”€ reports/               # Test reports
â”œâ”€â”€ resources/                      # Resource Management Testing
â”‚   â”œâ”€â”€ test_data/
â”‚   â”‚   â”œâ”€â”€ basic_resource_prompts.txt
â”‚   â”‚   â”œâ”€â”€ comprehensive_resource_prompts.txt
â”‚   â”‚   â”œâ”€â”€ ultra_complex_resource_prompts.txt
â”‚   â”‚   â””â”€â”€ resource_scenarios.json
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ run_resource_tests.py
â”‚   â”‚   â””â”€â”€ validate_resource_results.py
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ logs/                   # Test execution logs
â”‚       â”œâ”€â”€ payloads/              # Real API payloads
â”‚       â””â”€â”€ reports/               # Test reports
â”œâ”€â”€ multi_provider/                 # Multi-Provider Testing
â”‚   â”œâ”€â”€ test_data/
â”‚   â”‚   â””â”€â”€ provider_scenarios.json
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ test_all_providers.py
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ logs/
â”‚       â”œâ”€â”€ payloads/
â”‚       â””â”€â”€ reports/
â”œâ”€â”€ evidence/                       # Consolidated Evidence
â”‚   â”œâ”€â”€ api_payloads/              # All API evidence
â”‚   â”œâ”€â”€ test_reports/              # Generated reports
â”‚   â””â”€â”€ screenshots/               # Visual evidence
â””â”€â”€ shared/                        # Shared Testing Components
    â”œâ”€â”€ engines/                   # Core testing engines
    â”‚   â””â”€â”€ enhanced_real_mcp_integration_test.py  # Core test engine
    â””â”€â”€ utilities/                 # Shared utilities
        â”œâ”€â”€ generate_test_report.py    # Report generator
        â””â”€â”€ cleanup_test_data.py       # Data cleanup
```

## ðŸš€ Quick Start

### Basic Testing Commands

```bash
# Test integrations functionality
make test-integrations-basic-organized

# Test resources functionality  
make test-resources-basic-organized

# Run complete test suite
make test-complete-organized

# Generate test report
make generate-test-report-html
```

### Advanced Testing Commands

```bash
# Comprehensive integration tests
make test-integrations-advanced-organized

# Ultra-complex resource tests
make test-resources-ultra-organized

# Multi-provider comparison tests
make test-all-providers-organized

# Show test evidence
make show-test-evidence-organized
```

## ðŸ“Š Test Categories

### 1. Integration Management Tests

**Purpose**: Test the `integrations` tool functionality for managing OpsRamp integrations.

**Test Levels**:
- **Basic**: Simple integration queries (5-10 prompts)
- **Advanced**: Complex integration management scenarios (20-30 prompts)
- **All**: Complete integration test suite

**Key Features**:
- Real OpsRamp API integration (no mocks)
- Token-efficient prompts to avoid OpenAI limits
- Comprehensive API payload capture
- Integration status monitoring
- Configuration management testing

### 2. Resource Management Tests

**Purpose**: Test the `resources` tool functionality for managing OpsRamp resources.

**Test Levels**:
- **Basic**: Simple resource queries (5-10 prompts)
- **Comprehensive**: Detailed resource analysis (20-30 prompts)
- **Ultra**: Complex resource scenarios (50+ prompts)
- **All**: Complete resource test suite

**Key Features**:
- Real OpsRamp API integration (no mocks)
- Pagination and filtering support
- Resource discovery and inventory
- Performance metrics collection
- Hardware and software resource management

### 3. Multi-Provider Tests

**Purpose**: Compare AI model performance across different providers.

**Providers Tested**:
- OpenAI (GPT-3.5, GPT-4)
- Anthropic (Claude-3-Haiku, Claude-3-Sonnet)
- Google (Gemini-1.5-Flash, Gemini-1.5-Pro)

**Comparison Metrics**:
- Response accuracy
- Token efficiency
- Response time
- Error handling
- API call success rate

## ðŸ”§ Test Configuration

### Environment Setup

Ensure your `.env` file contains:
```bash
# OpenAI Configuration
OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-4

# Anthropic Configuration  
ANTHROPIC_API_KEY=sk-ant-api03-...
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# Google Configuration
GOOGLE_API_KEY=...
GOOGLE_MODEL=gemini-1.5-pro

# OpsRamp Configuration
OPSRAMP_API_KEY=...
OPSRAMP_TENANT_ID=...
OPSRAMP_BASE_URL=...
```

### Test Data Files

#### Integration Test Data
- `basic_integration_prompts.txt`: Token-efficient integration queries
- `advanced_integration_prompts.txt`: Complex integration scenarios
- `integration_scenarios.json`: Structured test scenarios

#### Resource Test Data
- `basic_resource_prompts.txt`: Simple resource queries
- `comprehensive_resource_prompts.txt`: Detailed resource analysis
- `ultra_complex_resource_prompts.txt`: Advanced resource scenarios
- `resource_scenarios.json`: Structured resource test scenarios

## ðŸ“ˆ Evidence Collection

### API Payload Evidence

All tests capture real API calls to OpsRamp:
- **Request payloads**: Complete API requests with headers
- **Response payloads**: Full API responses with data
- **Timing information**: Request/response timing
- **Error handling**: Failed requests and error responses

### Test Reports

Comprehensive reports are generated in multiple formats:
- **HTML**: Visual reports with charts and metrics
- **JSON**: Machine-readable test data
- **Text**: Simple text summaries

### Performance Metrics

- **Success Rate**: Percentage of successful tests
- **Response Time**: Average API response time
- **Token Efficiency**: Prompts processed per minute
- **Error Rate**: Failed requests and tool calls

## ðŸŽ¯ Testing Best Practices

### Token Management

To avoid OpenAI token limits:
1. Use pagination in resource queries
2. Request specific data subsets
3. Implement token-efficient prompts
4. Use alternative models for large datasets

### Real API Testing

All tests use real OpsRamp APIs:
- No mock data or simulated responses
- Actual production environment testing
- Real integration and resource data
- Authentic API error handling

### Evidence Preservation

- All API payloads are preserved as evidence
- Test logs include complete execution traces
- Reports show real performance metrics
- Screenshots capture visual evidence

## ðŸ” Troubleshooting

### Common Issues

1. **Token Limit Exceeded**
   - Use basic test variants
   - Implement pagination
   - Switch to Anthropic models

2. **API Connection Failures**
   - Check OpsRamp credentials
   - Verify network connectivity
   - Review API endpoint configuration

3. **Test Script Errors**
   - Check Python dependencies
   - Verify file permissions
   - Review log files for details

### Debug Commands

```bash
# Check server connectivity
make check-server

# Run single test for debugging
make test-single QUESTION="List first 3 integrations"

# Clean up test data
make cleanup-test-data-dry-organized

# Show detailed evidence
make show-test-evidence-organized
```

## ðŸ“‹ Test Execution Examples

### Integration Testing

```bash
# Basic integration test
cd tests && python integration/scripts/run_integration_tests.py --complexity basic

# Advanced integration test with specific model
cd tests && python integration/scripts/run_integration_tests.py --complexity advanced --model claude-3-sonnet-20240229
```

### Resource Testing

```bash
# Basic resource test
cd tests && python resources/scripts/run_resource_tests.py --complexity basic

# Comprehensive resource test with pagination
cd tests && python resources/scripts/run_resource_tests.py --complexity comprehensive --max-resources 50
```

### Report Generation

```bash
# Generate HTML report
cd tests && python scripts/generate_test_report.py --format html --period daily

# Generate JSON report for automation
cd tests && python scripts/generate_test_report.py --format json --period weekly
```

## ðŸŽ‰ Success Metrics

### Phase 1 Completion Criteria

- âœ… **Integration Tool**: Fully functional with real API
- âœ… **Resources Tool**: Fully functional with real API  
- âœ… **Test Framework**: Comprehensive testing infrastructure
- âœ… **Evidence Collection**: Real API payload capture
- âœ… **Multi-Provider Support**: OpenAI, Anthropic, Google
- âœ… **Token Management**: Efficient prompt handling
- âœ… **Report Generation**: Automated test reporting

### Quality Metrics

- **Success Rate**: > 95% for basic tests
- **Performance**: > 5 prompts/minute efficiency
- **Coverage**: Both integration and resource functionality
- **Evidence**: Complete API payload collection
- **Reliability**: Consistent test execution

## ðŸ”„ Continuous Integration

The testing framework supports:
- Automated test execution
- Scheduled test runs
- Performance monitoring
- Evidence archival
- Report generation
- Multi-provider comparison

## ðŸ“ž Support

For issues or questions:
1. Check the troubleshooting section
2. Review test logs in `output/logs/`
3. Examine API payloads in `evidence/api_payloads/`
4. Generate diagnostic reports
5. Contact the development team

---

**Note**: This testing framework uses real OpsRamp APIs and collects actual production data. All evidence is preserved for verification and compliance purposes. 